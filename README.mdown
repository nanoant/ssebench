# SSE matrix 4x4 multiplication benchmark

Based on article: [4x4 float matrix multiplication using SSE intrinsics](http://fhtr.blogspot.com/2010/02/4x4-float-matrix-multiplication-using.html)

### Usage

`make O=(2,3) TYPE=(0,1,2) CC=(gcc-4.6.2,clang-3.0,cc) ITER=100000000`

### Results 100M iterations

Tested on:

* **Mac OS X 10.7.3**
* **Intel Core i5 2400 2.5 GHz**

Results:

	             version  opt/-Ox   arch        plain  naive SSE    opt SSE
	     ICC [1] 12.1.3         3   corei7-avx   0.96       2.87       1.00
	     ICC     12.1.3         3   core2        1.04       3.20       1.05
	     GCC [2] 4.7            3   corei7-avx   0.73       2.08       0.54
	     GCC     4.7            3   core2        0.78       2.32       0.58
	     GCC     4.6.2          3   corei7-avx   0.88       1.97       0.54
	     GCC     4.6.2          3   core2        0.74       2.08       0.58
	   clang [3] 3.1/318.0.58   3   corei7-avx   3.03       3.19       1.03
	   clang     3.1/318.0.58   3   core2        3.03       3.39       1.03
	GCC/LLVM [4] 2.4.1/2336.1   3   core2        2.56       4.93     **CRASH**
	     GCC [5] 2.4.1          3   core2        2.70       4.41       1.63

[1] *ICC 12.1.3* binaries provided by [IntelÂ® C++ Composer XE 2011 for Mac OS X](http://software.intel.com/en-us/articles/intel-composer-xe/).
[2] *GCC 4.7* & *GCC 4.6.2* was built from [GNU sources](http://gcc.gnu.org/).
[3] *clang 3.1* binaries provided by *Xcode 4.3* package.
[4] *GCC/LLVM 2.4.1* binaries provided by *Xcode 4.2* package.
[5] *GCC 2.4.1* binaries provided by *Xcode 3.2.6* package.

## GCC seems to provide best performance

Both *GCC 4.7* and *4.6.2* provide best performance, also pure C implementation is just 50% slower under *GCC* than optimized SSE matrix multiplication, where it is 100% slower under *ICC* and 300% slower under *Clang*.

## Why ICC sucks in comparison to GCC for SSE optimized example

*GCC* can perform whole loop of operations using XMM (SSE) registers only if all calculation can be done in place. *ICC* moves values back and forth between stack and XMM registers using SSE4/AVX instructions.

## Why Clang sucks in comparison to GCC & ICC

*Clang* does not even use SSE4/AVX instructions, but older `movss` than moves single SSE register value. So it is even slower than ICC. *Clang* also poorly optimize pure C version, where **GCC** and **ICC** can optimize using AVX.

*Clang* 3.1 optimized SSE exampel loop:

	LBB0_5:                                 ##   Parent Loop BB0_4 Depth=1
	                                        ## =>  This Inner Loop Header: Depth=2
		movss	-96(%rbp,%rcx,4), %xmm4
		pshufd	$0, %xmm4, %xmm4        ## xmm4 = xmm4[0,0,0,0]
		mulps	%xmm3, %xmm4
		movss	-92(%rbp,%rcx,4), %xmm5
		pshufd	$0, %xmm5, %xmm5        ## xmm5 = xmm5[0,0,0,0]
		mulps	%xmm0, %xmm5
		addps	%xmm4, %xmm5
		movss	-88(%rbp,%rcx,4), %xmm4
		pshufd	$0, %xmm4, %xmm4        ## xmm4 = xmm4[0,0,0,0]
		mulps	%xmm1, %xmm4
		addps	%xmm5, %xmm4
		movss	-84(%rbp,%rcx,4), %xmm5
		pshufd	$0, %xmm5, %xmm5        ## xmm5 = xmm5[0,0,0,0]
		mulps	%xmm2, %xmm5
		addps	%xmm4, %xmm5
		movaps	%xmm5, -192(%rbp,%rcx,4)
		leaq	4(%rcx), %rcx
		cmpl	$16, %ecx
		jl	LBB0_5

*GCC* 4.6.2 loop:

	L6:
		movaps	%xmm2, %xmm9
	L3:
		movaps	%xmm1, %xmm8
		movaps	%xmm9, %xmm4
		movaps	%xmm0, %xmm2
		mulps	%xmm3, %xmm8
		movaps	%xmm6, %xmm5
		addq	$1, %rdx
		mulps	%xmm3, %xmm4
		cmpq	%rax, %rdx
		mulps	%xmm10, %xmm2
		mulps	%xmm3, %xmm5
		movaps	%xmm8, %xmm7
		addps	%xmm4, %xmm7
		addps	%xmm4, %xmm1
		movaps	%xmm0, %xmm4
		movaps	%xmm8, %xmm0
		mulps	%xmm3, %xmm4
		addps	%xmm9, %xmm0
		addps	%xmm7, %xmm2
		addps	%xmm4, %xmm1
		addps	%xmm4, %xmm0
		addps	%xmm7, %xmm4
		addps	%xmm5, %xmm2
		addps	%xmm5, %xmm1
		addps	%xmm5, %xmm0
		addps	%xmm4, %xmm6
		jl	L6
		movaps	%xmm2, (%rsp)
		movaps	%xmm1, 16(%rsp)
		movaps	%xmm0, 32(%rsp)
		movaps	%xmm6, 48(%rsp)
